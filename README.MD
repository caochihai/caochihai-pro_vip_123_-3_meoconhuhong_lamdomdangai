# ğŸš€ PDF Summarizer API with AI Power ğŸ§ ğŸ”¥

[![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-API-412991?style=for-the-badge&logo=openai)](https://platform.openai.com/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python)](https://www.python.org/)
[![License](https://img.shields.io/github/license/yourusername/pdf-summarizer?style=for-the-badge)](LICENSE)

## ğŸŒŸ Overview

This **PDF Summarizer API** extracts text from a given PDF URL and uses a powerful LLM (Large Language Model) to generate concise, meaningful summaries. Built with **FastAPI**, it supports both **synchronous** and **streaming** responses for real-time interaction.

Perfect for quickly understanding large documents like research papers, contracts, or reports.

---

## ğŸ§° Features

- ğŸ” Extracts text from remote PDF URLs
- ğŸ“ Automatically truncates input to fit model token limits
- ğŸ’¬ Summarizes content using advanced LLMs (Qwen, GPT, etc.)
- âš¡ Caching for faster repeated queries
- ğŸ”„ Streaming support for real-time response
- ğŸ›¡ï¸ Built with FastAPI â€“ fast, async-ready, and auto-documented

---

## ğŸš€ Quick Start

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/pdf-summarizer.git
cd pdf-summarizer
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up environment variables

Create a `.env` file in the root directory:

```env
MODEL_NAME=openai/cpatonn/Qwen3-4B-Instruct-2507-AWQ
API_BASE=http://1.53.58.232:8521/v1
API_KEY=EMPTY
MAX_TOKEN_INPUT=31000
MAX_TOKEN_OUTPUT=5000
TEMPERATURE=0.2
TOP_P=0.8
TOP_K=20
STREAM=true
PORT=5000
HOST=0.0.0.0
```

### 4. Add your system prompt

Make sure you have a `prompt_system.md` file in the root directory to guide the summarization behavior.

### 5. Run the app

```bash
uvicorn main:app --host 0.0.0.0 --port 5000
```

Or using the script:

```bash
python main.py
```

---

## ğŸ§ª API Endpoints

### ğŸ”¹ POST `/summarize_pdf`

Summarize a PDF from a URL.

**Request:**

```json
{
  "pdf_url": "https://example.com/sample.pdf"
}
```

**Response:**

```json
{
  "summary": "This is a concise summary of the PDF..."
}
```

---

### ğŸ”¹ POST `/stream_summary`

Stream a summary in real-time using Server-Sent Events (SSE).

**Request:**

```json
{
  "pdf_url": "https://example.com/sample.pdf"
}
```

**Response:**

> Streams tokens in real-time via `text/event-stream`.

---

## ğŸ§  Supported Models

You can configure any LiteLLM-compatible model:

- Qwen3 4B (default)
- GPT-4, GPT-3.5-turbo
- Llama 3, Mistral, and more

---

## ğŸ› ï¸ Requirements

- Python 3.8+
- FastAPI
- Uvicorn
- pdfplumber
- requests
- tiktoken
- litellm
- cachetools
- python-dotenv

Install via:

```bash
pip install -r requirements.txt
```

---

## ğŸ§¾ Example `.env`

```env
MODEL_NAME=openai/cpatonn/Qwen3-4B-Instruct-2507-AWQ
API_BASE=http://1.53.58.232:8521/v1
API_KEY=EMPTY
MAX_TOKEN_INPUT=31000
MAX_TOKEN_OUTPUT=5000
TEMPERATURE=0.2
TOP_P=0.8
TOP_K=20
STREAM=true
PORT=5000
HOST=0.0.0.0
```

---

## ğŸ“œ License

This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.

---

## ğŸ™Œ Contributing

Contributions are welcome! Please fork the repo and submit a pull request.

---

## ğŸ“¬ Contact

Have questions or feedback? Reach out to [your-email@example.com](mailto:your-email@example.com)

---

## ğŸŒˆ Made with â¤ï¸ using FastAPI & AI